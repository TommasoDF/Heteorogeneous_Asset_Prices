{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tomma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from datetime import datetime\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataframe\n",
    "df = pd.read_csv(\"results\\Bitcoin_from_2021-12-22_23-59-49_to_2021-10-04_18-01-43.csv\")\n",
    "df = df.set_index('Datetime')\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Cleaning definition\n",
    "pat1 = r'@[A-Za-z0-9]+' # this is to remove any text with @....\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'  # this is to remove the urls\n",
    "combined_pat = r'|'.join((pat1, pat2)) \n",
    "pat3 = r'[^a-zA-Z]' # to remove every other character except a-z & A-Z\n",
    "combined_pat2 = r'|'.join((combined_pat,pat3)) # we combine pat1, pat2 and pat3 to pass it in the cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 10000 of 1000001 has been processed\n",
      "Tweets 20000 of 1000001 has been processed\n",
      "Tweets 30000 of 1000001 has been processed\n",
      "Tweets 40000 of 1000001 has been processed\n",
      "Tweets 50000 of 1000001 has been processed\n",
      "Tweets 60000 of 1000001 has been processed\n",
      "Tweets 70000 of 1000001 has been processed\n",
      "Tweets 80000 of 1000001 has been processed\n",
      "Tweets 90000 of 1000001 has been processed\n",
      "Tweets 100000 of 1000001 has been processed\n",
      "Tweets 110000 of 1000001 has been processed\n",
      "Tweets 120000 of 1000001 has been processed\n",
      "Tweets 130000 of 1000001 has been processed\n",
      "Tweets 140000 of 1000001 has been processed\n",
      "Tweets 150000 of 1000001 has been processed\n",
      "Tweets 160000 of 1000001 has been processed\n",
      "Tweets 170000 of 1000001 has been processed\n",
      "Tweets 180000 of 1000001 has been processed\n",
      "Tweets 190000 of 1000001 has been processed\n",
      "Tweets 200000 of 1000001 has been processed\n",
      "Tweets 210000 of 1000001 has been processed\n",
      "Tweets 220000 of 1000001 has been processed\n",
      "Tweets 230000 of 1000001 has been processed\n",
      "Tweets 240000 of 1000001 has been processed\n",
      "Tweets 250000 of 1000001 has been processed\n",
      "Tweets 260000 of 1000001 has been processed\n",
      "Tweets 270000 of 1000001 has been processed\n",
      "Tweets 280000 of 1000001 has been processed\n",
      "Tweets 290000 of 1000001 has been processed\n",
      "Tweets 300000 of 1000001 has been processed\n",
      "Tweets 310000 of 1000001 has been processed\n",
      "Tweets 320000 of 1000001 has been processed\n",
      "Tweets 330000 of 1000001 has been processed\n",
      "Tweets 340000 of 1000001 has been processed\n",
      "Tweets 350000 of 1000001 has been processed\n",
      "Tweets 360000 of 1000001 has been processed\n",
      "Tweets 370000 of 1000001 has been processed\n",
      "Tweets 380000 of 1000001 has been processed\n",
      "Tweets 390000 of 1000001 has been processed\n",
      "Tweets 400000 of 1000001 has been processed\n",
      "Tweets 410000 of 1000001 has been processed\n",
      "Tweets 420000 of 1000001 has been processed\n",
      "Tweets 430000 of 1000001 has been processed\n",
      "Tweets 440000 of 1000001 has been processed\n",
      "Tweets 450000 of 1000001 has been processed\n",
      "Tweets 460000 of 1000001 has been processed\n",
      "Tweets 470000 of 1000001 has been processed\n",
      "Tweets 480000 of 1000001 has been processed\n",
      "Tweets 490000 of 1000001 has been processed\n",
      "Tweets 500000 of 1000001 has been processed\n",
      "Tweets 510000 of 1000001 has been processed\n",
      "Tweets 520000 of 1000001 has been processed\n",
      "Tweets 530000 of 1000001 has been processed\n",
      "Tweets 540000 of 1000001 has been processed\n",
      "Tweets 550000 of 1000001 has been processed\n",
      "Tweets 560000 of 1000001 has been processed\n",
      "Tweets 570000 of 1000001 has been processed\n",
      "Tweets 580000 of 1000001 has been processed\n",
      "Tweets 590000 of 1000001 has been processed\n",
      "Tweets 600000 of 1000001 has been processed\n",
      "Tweets 610000 of 1000001 has been processed\n",
      "Tweets 620000 of 1000001 has been processed\n",
      "Tweets 630000 of 1000001 has been processed\n",
      "Tweets 640000 of 1000001 has been processed\n",
      "Tweets 650000 of 1000001 has been processed\n",
      "Tweets 660000 of 1000001 has been processed\n",
      "Tweets 670000 of 1000001 has been processed\n",
      "Tweets 680000 of 1000001 has been processed\n",
      "Tweets 690000 of 1000001 has been processed\n",
      "Tweets 700000 of 1000001 has been processed\n",
      "Tweets 710000 of 1000001 has been processed\n",
      "Tweets 720000 of 1000001 has been processed\n",
      "Tweets 730000 of 1000001 has been processed\n",
      "Tweets 740000 of 1000001 has been processed\n",
      "Tweets 750000 of 1000001 has been processed\n",
      "Tweets 760000 of 1000001 has been processed\n",
      "Tweets 770000 of 1000001 has been processed\n",
      "Tweets 780000 of 1000001 has been processed\n",
      "Tweets 790000 of 1000001 has been processed\n",
      "Tweets 800000 of 1000001 has been processed\n",
      "Tweets 810000 of 1000001 has been processed\n",
      "Tweets 820000 of 1000001 has been processed\n",
      "Tweets 830000 of 1000001 has been processed\n",
      "Tweets 840000 of 1000001 has been processed\n",
      "Tweets 850000 of 1000001 has been processed\n",
      "Tweets 860000 of 1000001 has been processed\n",
      "Tweets 870000 of 1000001 has been processed\n",
      "Tweets 880000 of 1000001 has been processed\n",
      "Tweets 890000 of 1000001 has been processed\n",
      "Tweets 900000 of 1000001 has been processed\n",
      "Tweets 910000 of 1000001 has been processed\n",
      "Tweets 920000 of 1000001 has been processed\n",
      "Tweets 930000 of 1000001 has been processed\n",
      "Tweets 940000 of 1000001 has been processed\n",
      "Tweets 950000 of 1000001 has been processed\n",
      "Tweets 960000 of 1000001 has been processed\n",
      "Tweets 970000 of 1000001 has been processed\n",
      "Tweets 980000 of 1000001 has been processed\n",
      "Tweets 990000 of 1000001 has been processed\n",
      "Tweets 1000000 of 1000001 has been processed\n"
     ]
    }
   ],
   "source": [
    "### Starting the cleaning\n",
    "ps = PorterStemmer()\n",
    "cleaned_tweets = []\n",
    "for i in range(0, len(df)) :\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, len(df['Text']) ))\n",
    "    tweets = re.sub(combined_pat2,' ',df['Text'][i])\n",
    "    tweets = tweets.lower()\n",
    "    tweets = tweets.split()\n",
    "    tweets = [ps.stem(word) for word in tweets if not word in set(stopwords.words('english'))]\n",
    "    tweets = ' '.join(tweets)\n",
    "    cleaned_tweets.append(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the score\n",
    "df['cleaned_tweets'] = np.array(cleaned_tweets)\n",
    "scores = df['cleaned_tweets'].apply(vader.polarity_scores).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores, index=df.index)# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news = df.join(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset with only the compound senitment and the datetime\n",
    "compact_df = pd.DataFrame({'compound': parsed_and_scored_news['compound']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamp column to datetime objects\n",
    "extended_dates = pd.DataFrame({'timestamp': pd.to_datetime(compact_df.index, errors='coerce')})\n",
    "extended_dates['timestamp'].fillna(\"2099-01-01 00:00:00\", inplace=True)\n",
    "dates = extended_dates['timestamp'].dt.date.apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the final df by grouping for dates\n",
    "compact_df = pd.DataFrame({'compound': compact_df})\n",
    "compact_df['date'] = np.array(dates)\n",
    "final_df = compact_df.groupby(['date'])['compound'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb81384846b9b7f8d2f55ea8cfb80d355ddcf9934f98e7e175044fde44492714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
